{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3afdd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging \n",
    "import matplotlib.pyplot as plt \n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc2855ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d276c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x,leaky=False,negative_slope=None): # leaky is a flag for leaky relu, negative slope for leaky relu. \n",
    "    if leaky and negative_slope!=None:\n",
    "        try:\n",
    "            if negative_slope>0:\n",
    "                raise ValueError(\"Negative slope accepts integers only.\")            \n",
    "            return np.where(x<0,x*negative_slope,x)\n",
    "        \n",
    "        except ValueError as ve:\n",
    "            print(f\"an unexpected error{ve}\")\n",
    "            \n",
    "        except Exception as e:            \n",
    "            print(f\"An unexpected error {e}\")\n",
    "            raise \n",
    "    return np.maximum(0,x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb08b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_ReLU(x):\n",
    "    return np.where(x>0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b0a6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_initialization(fraction,weight_size):\n",
    "    fract=np.sqrt(2/fraction)\n",
    "    return np.random.normal(0,fract,size=weight_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc0b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_initialization(fraction,weight_size):\n",
    "    fract= np.sqrt(6/fraction)\n",
    "    return np.random.uniform(-fract,fract,size=weight_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4799099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_map={\n",
    "        \"uniform\":uniform_initialization,\n",
    "        \"normal\":normal_initialization\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cae85278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_layer(layer_in,layer_out,activation=\"linear\",mode=\"he\",method=\"uniform\",bias=True):\n",
    "    \"\"\" Returns weights and biases initialized as response to input information\n",
    "\n",
    "    Args:\n",
    "        layer_in (_type_): _description_\n",
    "        layer_out (_type_): _description_\n",
    "        mode (str, optional): _description_. Defaults to \"he\".\n",
    "        method (str, optional): _description_. Defaults to \"uniform\".\n",
    "\n",
    "    Raises:\n",
    "        ValueError: _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    weights=np.zeros((layer_in,layer_out))\n",
    "    if bias:\n",
    "        biases=np.zeros((1,layer_out))\n",
    "    \n",
    "    \n",
    "    try:        \n",
    "        calcluation=0\n",
    "        \n",
    "        if mode.lower()=='random': return np.random.random(size=(layer_in,layer_out))\n",
    "        \n",
    "        elif mode.lower()==\"he\":\n",
    "            calcluation=1/(np.sqrt(layer_in+layer_out))\n",
    "            \n",
    "        elif mode.lower()==\"xavier\":\n",
    "            calcluation=1/(np.sqrt(layer_in))\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Only accepts 'random','he' and 'xavier' string as arguments.\")\n",
    "\n",
    "        weights=method_map[method](calcluation,(layer_in,layer_out))\n",
    "        \n",
    "        \n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"Error occured: {e}\")    \n",
    "\n",
    "    \n",
    "    return {\"weight\":weights,\"bias\":biases,\"activation\":activation}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c97ee77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.76644102, -2.97097569,  3.55649058],\n",
       "       [-3.5330374 , -1.85551649,  2.40429865]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer=(2,3)\n",
    "val=initialize_layer(2,3,method=\"uniform\")\n",
    "val['weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b4ca4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_hidden_layers=[\n",
    "    (8,\"relu\"),\n",
    "    (4,\"relu\"),\n",
    "    (2,\"relu\"),\n",
    "    (1,\"linear\")\n",
    "    ]\n",
    "custom_X=np.array([[1,2,3],[4,5,6],[7,8,9],[10,11,12],[13,14,15]]) # 5 x 3 \n",
    "custom_Y=np.array([1,2,3,4,5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1b0173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc369af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(hidden_layers,X):\n",
    "    neural_network={}\n",
    "    new_hidden=hidden_layers.copy()\n",
    "    new_hidden.insert(0,(X.shape[1],\"relu\"))\n",
    "    \n",
    "    for layer in range(len(hidden_layers)):\n",
    "        neural_network[f\"Layer{layer}\"]=initialize_layer(new_hidden[layer][0],new_hidden[layer+1][0],activation=new_hidden[layer+1][1])\n",
    "    return neural_network\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ce88ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Layer0': {'weight': array([[ 1.04116575,  2.15237706,  3.59967635, -1.76577998,  4.22794401,\n",
       "           0.15562409,  2.1893795 , -3.3941601 ],\n",
       "         [ 0.66064513,  1.98954149,  4.08141815,  0.15174739,  3.6854873 ,\n",
       "           2.71298353, -2.64028752,  1.17554755],\n",
       "         [-1.03757976, -0.74582748, -4.17602691,  3.6918322 ,  1.4927271 ,\n",
       "          -0.79258595,  0.1156259 , -3.20086966]]),\n",
       "  'bias': array([[0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       "  'activation': 'relu'},\n",
       " 'Layer1': {'weight': array([[-2.02734327,  2.82540746, -1.79601613, -1.19294662],\n",
       "         [ 4.05671061, -4.27405414, -1.48752994,  3.67603107],\n",
       "         [-4.14297448, -1.92521729,  3.42609232, -1.75662326],\n",
       "         [ 2.3989703 , -3.57874708, -0.96990567,  4.45932537],\n",
       "         [-4.43116923,  2.67143439, -0.21314367,  2.92919716],\n",
       "         [ 0.83684909, -0.68345343,  3.09800708, -0.35079169],\n",
       "         [ 0.70965904, -0.68028242, -3.50899395,  0.64713595],\n",
       "         [-2.5728746 , -3.36415233, -2.67990382, -0.53752185]]),\n",
       "  'bias': array([[0., 0., 0., 0.]]),\n",
       "  'activation': 'relu'},\n",
       " 'Layer2': {'weight': array([[ 0.1352096 , -3.30753968],\n",
       "         [ 3.40217774,  1.22309062],\n",
       "         [ 2.10575363,  3.54433409],\n",
       "         [-0.71361413, -1.76319174]]),\n",
       "  'bias': array([[0., 0.]]),\n",
       "  'activation': 'relu'},\n",
       " 'Layer3': {'weight': array([[-0.59199101],\n",
       "         [ 0.7339825 ]]),\n",
       "  'bias': array([[0.]]),\n",
       "  'activation': 'linear'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network=build_network(hidden_layers=custom_hidden_layers,X=custom_X)\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98c98b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': array([[ 1.04116575,  2.15237706,  3.59967635, -1.76577998,  4.22794401,\n",
       "          0.15562409,  2.1893795 , -3.3941601 ],\n",
       "        [ 0.66064513,  1.98954149,  4.08141815,  0.15174739,  3.6854873 ,\n",
       "          2.71298353, -2.64028752,  1.17554755],\n",
       "        [-1.03757976, -0.74582748, -4.17602691,  3.6918322 ,  1.4927271 ,\n",
       "         -0.79258595,  0.1156259 , -3.20086966]]),\n",
       " 'bias': array([[0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 'activation': 'relu'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network['Layer0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ba4ce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(input_network,X):\n",
    "    \n",
    "    \n",
    "    network=input_network.copy()\n",
    "    network_values={}\n",
    "    \n",
    "    result=X @ network['Layer0'][\"weight\"] + network['Layer0'][\"bias\"]\n",
    "    \n",
    "    network_values['Layer0']=[result]\n",
    "    \n",
    "    if network['Layer0']['activation']:\n",
    "        result=ReLU(result)\n",
    "        \n",
    "    network_values['Layer0'].append(result)\n",
    "    \n",
    "    network.pop('Layer0')\n",
    "    \n",
    "    count=1\n",
    "    for layer_name in network:\n",
    "        \n",
    "        params = network[layer_name]\n",
    "        W = params[\"weight\"]\n",
    "        b = params[\"bias\"]\n",
    "        result=result @ W +b\n",
    "        network_values[layer_name]=[result]\n",
    "        count+=1\n",
    "        \n",
    "        if params[\"activation\"].lower()==\"relu\":\n",
    "            result=ReLU(result)\n",
    "            \n",
    "        network_values[layer_name].append(result)\n",
    "        \n",
    "        print(f\"Result in layer {layer_name} : {result}\")\n",
    "    print(f\"Total calculations: {count}\")    \n",
    "    return network_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a795bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result in layer Layer1 : [[  0.           0.           0.         103.15193737]\n",
      " [  0.           0.          14.63303934 230.26525968]\n",
      " [  0.           0.          39.15901516 355.13871941]\n",
      " [  0.           0.          63.68499098 480.01217914]\n",
      " [  0.           0.          88.2109668  604.88563886]]\n",
      "Result in layer Layer2 : [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "Result in layer Layer3 : [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Total calculations: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Layer0': [array([[ -0.75028327,   3.89397759,  -0.76556808,   9.61321141,\n",
       "           16.07709991,   3.20383328,  -2.74431785, -10.64567398],\n",
       "         [  1.2424101 ,  14.08225078,   9.7496347 ,  15.84661027,\n",
       "           44.29557514,   9.43189827,  -3.75016422, -26.90412061],\n",
       "         [  3.23510346,  24.27052397,  20.26483748,  22.08000912,\n",
       "           72.51405036,  15.65996326,  -4.75601059, -43.16256724],\n",
       "         [  5.22779683,  34.45879716,  30.78004026,  28.31340797,\n",
       "          100.73252559,  21.88802825,  -5.76185696, -59.42101388],\n",
       "         [  7.22049019,  44.64707035,  41.29524304,  34.54680682,\n",
       "          128.95100081,  28.11609324,  -6.76770333, -75.67946051]]),\n",
       "  array([[  0.        ,   3.89397759,   0.        ,   9.61321141,\n",
       "           16.07709991,   3.20383328,   0.        ,   0.        ],\n",
       "         [  1.2424101 ,  14.08225078,   9.7496347 ,  15.84661027,\n",
       "           44.29557514,   9.43189827,   0.        ,   0.        ],\n",
       "         [  3.23510346,  24.27052397,  20.26483748,  22.08000912,\n",
       "           72.51405036,  15.65996326,   0.        ,   0.        ],\n",
       "         [  5.22779683,  34.45879716,  30.78004026,  28.31340797,\n",
       "          100.73252559,  21.88802825,   0.        ,   0.        ],\n",
       "         [  7.22049019,  44.64707035,  41.29524304,  34.54680682,\n",
       "          128.95100081,  28.11609324,   0.        ,   0.        ]])],\n",
       " 'Layer1': [array([[ -29.70067651,  -10.28707657,   -8.61755041,  103.15193737],\n",
       "         [-136.15622977,  -20.27270344,   14.63303934,  230.26525968],\n",
       "         [-247.30459377,  -29.61235927,   39.15901516,  355.13871941],\n",
       "         [-358.45295777,  -38.9520151 ,   63.68499098,  480.01217914],\n",
       "         [-469.60132177,  -48.29167093,   88.2109668 ,  604.88563886]]),\n",
       "  array([[  0.        ,   0.        ,   0.        , 103.15193737],\n",
       "         [  0.        ,   0.        ,  14.63303934, 230.26525968],\n",
       "         [  0.        ,   0.        ,  39.15901516, 355.13871941],\n",
       "         [  0.        ,   0.        ,  63.68499098, 480.01217914],\n",
       "         [  0.        ,   0.        ,  88.2109668 , 604.88563886]])],\n",
       " 'Layer2': [array([[ -73.61068022, -181.87664385],\n",
       "         [-133.50696764, -354.13742346],\n",
       "         [-170.97277057, -487.38502381],\n",
       "         [-208.43857351, -620.63262415],\n",
       "         [-245.90437644, -753.8802245 ]]),\n",
       "  array([[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]])],\n",
       " 'Layer3': [array([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]),\n",
       "  array([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]])]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_result=forward_pass(network,X=custom_X)\n",
    "forward_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87630ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2fa5e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true,y_pred,type='mse'):\n",
    "    y_true=y_true.reshape(-1,1)\n",
    "    y_pred=y_pred.reshape(-1,1)\n",
    "    # print(y_true.shape,y_pred.shape)\n",
    "    loss=np.sum((y_true-y_pred)**2)\n",
    "    return loss/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "119eb2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4af4ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_ReLU(activation):\n",
    "    return np.where(activation>0,1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f71a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_linear(activation):\n",
    "    return activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2d924db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_sigmoid(activation):\n",
    "    return activation * (1-activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "559e7f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_update(network,X,y,result):\n",
    "    \"\"\"\n",
    "    Docstring for gradient_update\n",
    "    \n",
    "    :param network: Network structure of our NN, needed for updating network parameter\n",
    "    :param X: For Forward pass\n",
    "    :param y: For calculating loss of prediction\n",
    "    :param result: Contains Activation and Linear combination values for all layers in forward pass\n",
    "    \"\"\"\n",
    "    activations=[activations for (Z,activations) in result.values()]\n",
    "    activations.insert(0,X) # for final input layer calculation streamlining \n",
    "    output=activations[-1].reshape(-1,1) # final value of activations is being used as output\n",
    "    y=y.reshape(-1,1)\n",
    "    \n",
    "    Z=[Z for (Z,activations) in result.values()]\n",
    "    \n",
    "    # Z, activations = map(list, zip(*result.values()))\n",
    "\n",
    "    #dL/dZ of previous layers stored, this will be backpropagated through all layers\n",
    "    # multiply by weights for getting gradients for weights, these will be default updates for biases\n",
    "    linear_error_diff=[] \n",
    "    \n",
    "    loss_value=loss_function(y,output) # considering MSE\n",
    "    print(f\"Current loss: {loss_value}\")\n",
    "    \n",
    "    if network[f'Layer{len(network)-1}']['activation'].lower()==\"relu\":\n",
    "        activation_grad=gradient_ReLU(Z[-1])\n",
    "    else: # currently only considering linear and relu, if not relu then linear which is the same \n",
    "        activation_grad=1\n",
    "        \n",
    "    output_loss_diff= output - y # y_pred - y_true , MSE differential for final linear layer\n",
    "    \n",
    "    loss_by_Z=output_loss_diff * activation_grad\n",
    "    linear_error_diff.append(loss_by_Z)\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    for i in range(1,len(activations)+1):\n",
    "        \n",
    "        current_layer_activation=network[f'Layer{len(network)-i}']['activation'].lower()\n",
    "        if current_layer_activation=='relu':\n",
    "            activation_grad=gradient_ReLU(activations[-(i+1)]) #current layer activation grad\n",
    "        \n",
    "        forward_layer_error=linear_error_diff[0] # dl/dz for forward layer\n",
    "        forward_layer_weight=network[f\"Layer{len(network)-i}\"][\"weight\"].T # Transposed the weights of forward layer\n",
    "        \n",
    "        print(\"fle\",forward_layer_error.shape)\n",
    "        print(\"flw\",forward_layer_weight.shape)\n",
    "        # print(\"flw\",activation_grad)\n",
    "        \n",
    "        error_Z_by_layer= forward_layer_error @ forward_layer_weight * activation_grad\n",
    "        print(\"le\",error_Z_by_layer)\n",
    "        #dL/dZ for current layer= dl/dZ of forward layer x weights of forward layer x relu grad/ grad of current layer activation\n",
    "        # for dL/dW of this layer, multiiply this by weights of this layer,\n",
    "        # fo dl/lB of this layer, multiply this by bias of this layer\n",
    "        # for backprop, element wise multiplication with activations, matrix multiplications with weights \n",
    "        linear_error_diff=[error_Z_by_layer] + linear_error_diff\n",
    "        # print(linear_error_diff, \"le array baybe\")\n",
    "    # print(len(network))\n",
    "    # print(len(result))\n",
    "    print(error_Z_by_layer,\" er / layer\")\n",
    "    return error_Z_by_layer\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a5843369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss: 27.5\n",
      "fle (5, 1)\n",
      "flw (1, 2)\n",
      "le [[ 0.59199101 -0.7339825 ]\n",
      " [ 1.18398202 -1.467965  ]\n",
      " [ 1.77597304 -2.2019475 ]\n",
      " [ 2.36796405 -2.93592999]\n",
      " [ 2.95995506 -3.66991249]]\n",
      "fle (5, 2)\n",
      "flw (2, 4)\n",
      "le [[ 0.          0.         -0.          0.87169873]\n",
      " [ 0.          0.         -2.70978394  1.74339745]\n",
      " [ 0.          0.         -4.06467591  2.61509618]\n",
      " [ 0.          0.         -5.41956789  3.4867949 ]\n",
      " [ 0.          0.         -6.77445986  4.35849363]]\n",
      "fle (5, 4)\n",
      "flw (4, 8)\n",
      "le [[ -0.           3.2043916   -0.           3.88718824   2.55337743\n",
      "   -0.30578467   0.          -0.        ]\n",
      " [  2.78703557  10.43966793 -12.34646249  10.40261128   5.68432816\n",
      "   -9.00649917   0.           0.        ]\n",
      " [  4.18055335  15.6595019  -18.51969373  15.60391693   8.52649225\n",
      "  -13.50974876   0.           0.        ]\n",
      " [  5.57407114  20.87933586 -24.69292497  20.80522257  11.36865633\n",
      "  -18.01299834   0.           0.        ]\n",
      " [  6.96758892  26.09916983 -30.86615622  26.00652821  14.21082041\n",
      "  -22.51624793   0.           0.        ]]\n",
      "fle (5, 8)\n",
      "flw (8, 3)\n",
      "le [[ 10.78108914  15.54599202  16.01477973]\n",
      " [-14.80873107 -29.68607773  94.90946748]\n",
      " [-22.21309661 -44.52911659 142.36420122]\n",
      " [-29.61746214 -59.37215546 189.81893496]\n",
      " [-37.02182768 -74.21519432 237.2736687 ]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Layer-1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgradient_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforward_result\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mgradient_update\u001b[39m\u001b[34m(network, X, y, result)\u001b[39m\n\u001b[32m     34\u001b[39m linear_error_diff.append(loss_by_Z)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(activations)+\u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     current_layer_activation=\u001b[43mnetwork\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLayer\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m)\u001b[49m\u001b[43m-\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mactivation\u001b[39m\u001b[33m'\u001b[39m].lower()\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m current_layer_activation==\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     43\u001b[39m         activation_grad=gradient_ReLU(activations[-(i+\u001b[32m1\u001b[39m)]) \u001b[38;5;66;03m#current layer activation grad\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'Layer-1'"
     ]
    }
   ],
   "source": [
    "gradient_update(network=network,X=custom_X,y=custom_Y,result=forward_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06be2eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebacffdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
